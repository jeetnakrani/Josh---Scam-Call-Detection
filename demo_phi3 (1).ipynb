{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynDFcnktWKDX",
        "outputId": "2cebe115-d732-4f8a-bf4e-a4d8dd2bbb99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q gradio\n",
        "!pip install -q git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # -*- coding: utf-8 -*-\n",
        "# \"\"\"OpenAI Whisper from Hugging Face Transformers with Microsoft PHI 3 Integration\"\"\"\n",
        "\n",
        "# import gradio as gr\n",
        "# from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "# import torch\n",
        "# from huggingface_hub import InferenceClient\n",
        "# import os\n",
        "\n",
        "# # Initialize the InferenceClient for PHI 3\n",
        "# client = InferenceClient(\n",
        "#     \"microsoft/phi-3\",  # Update this to the correct model name for PHI 3\n",
        "#     token=os.getenv(\"HF_API_TOKEN\")\n",
        "# )\n",
        "\n",
        "# # Check if a GPU is available and use it if possible\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# # Initialize the Whisper pipeline\n",
        "# whisper = pipeline('automatic-speech-recognition', model='openai/whisper-tiny', device=0 if device == 'cuda' else -1)\n",
        "\n",
        "# # Load Microsoft PHI 3 model and tokenizer (Assuming same method as PHI 1.5)\n",
        "# instructions = os.getenv(\"INST\")\n",
        "\n",
        "\n",
        "# def query_phi(prompt):\n",
        "#     response = \"\"  # Initialize an empty string to store the response\n",
        "#     for message in client.chat_completion(\n",
        "#       messages=[{\"role\": \"user\", \"content\": f\"{instructions}\\n{prompt}\"}],\n",
        "#       max_tokens=100,\n",
        "#       stream=True,\n",
        "#     ):\n",
        "#         response += message.choices[0].delta.content  # Append each message to the response\n",
        "#     return response  # Return the accumulated response after the loop\n",
        "\n",
        "\n",
        "# def transcribe_and_query(audio):\n",
        "#     # Transcribe the audio file\n",
        "#     transcription = whisper(audio)[\"text\"]\n",
        "#     transcription = \"Prompt : \" + transcription\n",
        "#     # Query Microsoft PHI 3 with the transcribed text\n",
        "#     phi_response = query_phi(transcription)\n",
        "\n",
        "#     return transcription, phi_response\n",
        "\n",
        "# # Create Gradio interface\n",
        "# iface = gr.Interface(\n",
        "#     fn=transcribe_and_query,\n",
        "#     inputs=gr.Audio(type=\"filepath\"),\n",
        "#     outputs=[\"text\", \"text\"],\n",
        "#     title=\"Scam Call detector with BEEP\",\n",
        "#     description=\"Upload your recorded call to see if it is a scam or not. /n Stay Safe, Stay Secure.\"\n",
        "# )\n",
        "\n",
        "# # Launch the interface\n",
        "# iface.launch()\n"
      ],
      "metadata": {
        "id": "qcJUkrrjWL1i"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # -*- coding: utf-8 -*-\n",
        "# \"\"\"OpenAI Whisper from Hugging Face Transformers with Microsoft PHI 3 Integration\"\"\"\n",
        "\n",
        "# import gradio as gr\n",
        "# from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "# import torch\n",
        "# from huggingface_hub import InferenceClient\n",
        "# import os\n",
        "\n",
        "# # Initialize the InferenceClient for PHI 3\n",
        "# client = InferenceClient(\n",
        "#     \"microsoft/phi-2\",  # Update this to the correct model name for PHI 3\n",
        "#     token=os.getenv(\"HF_API_TOKEN\")\n",
        "# )\n",
        "\n",
        "# # Check if a GPU is available and use it if possible\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# # Initialize the Whisper pipeline\n",
        "# whisper = pipeline('automatic-speech-recognition', model='openai/whisper-tiny', device=0 if device == 'cuda' else -1)\n",
        "\n",
        "# # Load Microsoft PHI 3 model and tokenizer (Assuming same method as PHI 1.5)\n",
        "# instructions = os.getenv(\"INST\")\n",
        "\n",
        "\n",
        "# def query_phi(prompt):\n",
        "#     response = \"\"  # Initialize an empty string to store the response\n",
        "#     for message in client.chat_completion(\n",
        "#       messages=[{\"role\": \"user\", \"content\": f\"{instructions}\\n{prompt}\"}],\n",
        "#       max_tokens=200,\n",
        "#       stream=True,\n",
        "#     ):\n",
        "#         response += message.choices[0].delta.content  # Append each message to the response\n",
        "#     return response  # Return the accumulated response after the loop\n",
        "\n",
        "\n",
        "# def transcribe_and_query(audio):\n",
        "#     # Transcribe the audio file\n",
        "#     transcription = whisper(audio)[\"text\"]\n",
        "#     transcription = \"Prompt : \" + transcription\n",
        "#     # Query Microsoft PHI 3 with the transcribed text\n",
        "#     phi_response = query_phi(transcription)\n",
        "\n",
        "#     # Here we can analyze the PHI response to determine if the call is a scam or not.\n",
        "#     # For simplicity, let's assume the response contains the word \"scam\" or \"fraud\" for scam detection.\n",
        "#     if \"scam\" in phi_response.lower() or \"fraud\" in phi_response.lower():\n",
        "#         return \"Scam\"\n",
        "#     else:\n",
        "#         return \"Not Scam\"\n",
        "\n",
        "# # Create Gradio interface\n",
        "# iface = gr.Interface(\n",
        "#     fn=transcribe_and_query,\n",
        "#     inputs=gr.Audio(type=\"filepath\"),\n",
        "#     outputs=\"text\",  # Just a single output to indicate if it is a scam or not\n",
        "#     title=\"Scam Call Detector\",\n",
        "#     description=\"Upload your recorded call to see if it is a scam or not. Stay Safe, Stay Secure.\"\n",
        "# )\n",
        "\n",
        "# # Launch the interface\n",
        "# iface.launch()\n"
      ],
      "metadata": {
        "id": "zDZv5LYdWL5F"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "alS0B6p9WKOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mXR7loqJWKQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jqJ09t9GWKSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ixxQF07YZ4sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"OpenAI Whisper from Hugging Face Transformers with Microsoft PHI 2 Integration\"\"\"\n",
        "\n",
        "import gradio as gr\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from huggingface_hub import InferenceClient\n",
        "import os\n",
        "\n",
        "client = InferenceClient(\n",
        "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "    token = os.getenv(\"HF_API_TOKEN\")\n",
        ")\n",
        "\n",
        "\n",
        "# Check if a GPU is available and use it if possible\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Initialize the Whisper pipeline\n",
        "whisper = pipeline('automatic-speech-recognition', model='openai/whisper-tiny', device=0 if device == 'cuda' else -1)\n",
        "\n",
        "# Load Microsoft PHI 1.5 model and tokenizer\n",
        "\n",
        "\n",
        "instructions = os.getenv(\"INST\")\n",
        "\n",
        "\n",
        "def query_phi(prompt):\n",
        "    response = \"\"  # Initialize an empty string to store the response\n",
        "    for message in client.chat_completion(\n",
        "      messages=[{\"role\": \"user\", \"content\": f\"{instructions}\\n{prompt}\"}],\n",
        "      max_tokens=500,\n",
        "      stream=True,\n",
        "    ):\n",
        "        response += message.choices[0].delta.content  # Append each message to the response\n",
        "    return response  # Return the accumulated response after the loop\n",
        "\n",
        "\n",
        "def transcribe_and_query(audio):\n",
        "    # Transcribe the audio file\n",
        "    transcription = whisper(audio)[\"text\"]\n",
        "    transcription = \"Prompt : \" + transcription\n",
        "    # Query Microsoft PHI 1.5 with the transcribed text\n",
        "    phi_response = query_phi(transcription)\n",
        "\n",
        "    return transcription, phi_response\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=transcribe_and_query,\n",
        "    inputs=gr.Audio(type=\"filepath\"),\n",
        "    outputs=[\"text\", \"text\"],\n",
        "    title=\"Scam Call detector with BEEP\",\n",
        "    description=\"Upload an your recorded call to see if it is a scam or not. /n Stay Safe, Stay Secure.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "vl7ZQSAHZ4ur",
        "outputId": "1251aef1-e91c-4186-c3fd-25d499e05471"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8d130cfe993df9648e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8d130cfe993df9648e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6xG-kTLAZ4wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M3fUiXWCZ4yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z3ibrkx_Z4z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U8ydTBmBZ41j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ilX8t5G6Z441"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f1zzXIjNZ48D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wffSz0OAWKVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sh-9JXM6WL7V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ffnbzAXaWL9r"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"OpenAI Whisper from Hugging Face Transformers with Microsoft PHI 3 Integration\"\"\"\n",
        "\n",
        "import torch\n",
        "from huggingface_hub import InferenceClient\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Initialize the InferenceClient for PHI 3\n",
        "client = InferenceClient(\n",
        "    \"microsoft/phi-3\",  # Ensure this is the correct model name for PHI 3\n",
        "    token=os.getenv(\"HF_API_TOKEN\")\n",
        ")\n",
        "\n",
        "# Check if a GPU is available and use it if possible\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Initialize the Whisper pipeline\n",
        "from transformers import pipeline\n",
        "whisper = pipeline('automatic-speech-recognition', model='openai/whisper-tiny', device=0 if device == 'cuda' else -1)\n",
        "\n",
        "# Load Microsoft PHI 3 model and tokenizer\n",
        "instructions = os.getenv(\"INST\")\n",
        "\n",
        "def query_phi(prompt):\n",
        "    response = \"\"  # Initialize an empty string to store the response\n",
        "    for message in client.chat_completion(\n",
        "      messages=[{\"role\": \"user\", \"content\": f\"{instructions}\\n{prompt}\"}],\n",
        "      max_tokens=500,\n",
        "      stream=True,\n",
        "    ):\n",
        "        response += message.choices[0].delta.content  # Append each message to the response\n",
        "    return response  # Return the accumulated response after the loop\n",
        "\n",
        "def transcribe_and_query(audio):\n",
        "    # Transcribe the audio file\n",
        "    transcription = whisper(audio)[\"text\"]\n",
        "    transcription = \"Prompt : \" + transcription\n",
        "    # Query Microsoft PHI 3 with the transcribed text\n",
        "    phi_response = query_phi(transcription)\n",
        "\n",
        "    return transcription, phi_response\n",
        "\n",
        "# Save models and functions\n",
        "def save_models():\n",
        "    # Save the Whisper model\n",
        "    with open('whisper_model.pkl', 'wb') as f:\n",
        "        pickle.dump(whisper, f)\n",
        "\n",
        "    # Save the PHI 3 client and query function\n",
        "    with open('phi3_client.pkl', 'wb') as f:\n",
        "        pickle.dump(client, f)\n",
        "\n",
        "    # Save the instructions if required\n",
        "    with open('instructions.txt', 'w') as f:\n",
        "        f.write(instructions)\n",
        "\n",
        "    print(\"Models and components saved.\")\n",
        "\n",
        "# Call this function to save the model and other components\n",
        "save_models()\n",
        "\n",
        "# Example for loading and using saved models:\n",
        "\n",
        "# Load saved Whisper model\n",
        "with open('whisper_model.pkl', 'rb') as f:\n",
        "    whisper_loaded = pickle.load(f)\n",
        "\n",
        "# Load saved PHI 3 client\n",
        "with open('phi3_client.pkl', 'rb') as f:\n",
        "    phi3_client_loaded = pickle.load(f)\n",
        "\n",
        "# Load saved instructions\n",
        "with open('instructions.txt', 'r') as f:\n",
        "    instructions_loaded = f.read()\n",
        "\n",
        "# Now you can use these saved models to transcribe audio and query PHI 3 for detection:\n",
        "audio_file = 'path_to_audio.wav'  # Replace with your audio file path\n",
        "transcription, phi_response = transcribe_and_query(audio_file)\n",
        "print(\"Transcription:\", transcription)\n",
        "print(\"PHI 3 Response:\", phi_response)\n"
      ],
      "metadata": {
        "id": "qbLjTn1zWMAB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "5daf39b7-f027-41ae-b27d-c63920f5b639"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "write() argument must be str, not None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e1894b7b02b1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Call this function to save the model and other components\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0msave_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Example for loading and using saved models:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-e1894b7b02b1>\u001b[0m in \u001b[0;36msave_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Save the instructions if required\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'instructions.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Models and components saved.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not None"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DM20m3_vWMCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5KLlIB10WMEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rsgYoPnbWPl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4yrwo7KOWMHM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}